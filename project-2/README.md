# Netflix Revenue & Subscriber Analytics

This project demonstrates an end‑to‑end data‑engineering and data‑science
workflow on a publicly available Netflix revenue dataset.  It is designed to
showcase skills in ETL, data modelling, exploratory analysis, forecasting,
dashboard development and basic DevOps.

## Dataset

The data set (`data/netflix_revenue_updated.csv`) contains quarterly metrics
for Netflix from **Q1 2019 through Q1 2024**.  For each quarter it records:

* Global streaming revenue
* Regional streaming revenue for the **U.S./Canada (UCAN)**, **EMEA**, **Latin America (LATM)** and **Asia–Pacific (APAC)** regions
* Streaming membership counts by region
* Average Revenue Per User (ARPU) by region
* Total streaming memberships

Industry data show that the U.S./Canada region remains Netflix’s largest
revenue generator (about **44 % of all revenue in recent years**)【482364198331002†L582-L595】, while the **EMEA region now has the most subscribers**
(~93.9 million in 2024)【482364198331002†L690-L704】.  The U.S./Canada market also generates the **highest ARPU** (≈ $17.17 per user in 2024)
and Asia–Pacific the lowest【482364198331002†L601-L613】.

## Project structure

```
project-1/
├── data/
│   ├── netflix_revenue_updated.csv  # raw dataset
│   └── netflix.db                   # SQLite database created by ETL pipeline
├── etl/
│   └── etl_pipeline.py             # reads CSV, transforms and loads into SQLite
├── notebooks/
│   └── eda_forecasting.py          # EDA & forecasting script
├── app/
│   └── streamlit_app.py            # interactive dashboard
├── outputs/
│   ├── revenue_trends.png          # generated by analysis script
│   ├── membership_trends.png
│   ├── arpu_trends.png
│   └── global_revenue_forecast.png
├── requirements.txt                # Python dependencies
└── README.md                       # project documentation (this file)
```

## Quick start

1. **Install dependencies**

   Create a virtual environment (optional) and install required packages:

   ```bash
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Run the ETL pipeline**

   The ETL script will read the CSV file, unpivot regional data, create a
   star‑schema and load everything into a local SQLite database (`data/netflix.db`):

   ```bash
   python etl/etl_pipeline.py
   ```

3. **Perform analysis and generate forecasts**

   Execute the analysis script to produce summary plots and a simple ARIMA
   forecast for global streaming revenue.  Plots are saved into the
   `outputs/` directory.

   ```bash
   python notebooks/eda_forecasting.py
   ```

4. **Launch the dashboard**

   Run the Streamlit app to explore the data interactively.  Select a
   region and metric (revenue, memberships or ARPU) and view the
   corresponding trend.  A global revenue forecast is also displayed.

   ```bash
   streamlit run app/streamlit_app.py
   ```

## About this project

The objective is to illustrate a full analytics workflow:

* **Data engineering** – A Python ETL pipeline (orchestrated via a simple
  script, but ready to be scheduled in Airflow) cleans and unpivots the
  quarterly revenue data and loads it into a properly modelled database.
* **Exploratory analysis** – Python scripts generate descriptive charts and
  compute growth metrics, highlighting regional patterns.  For example,
  global streaming revenue climbed from about **$10 billion in 2019** to over
  **$17 billion in 2024** in the U.S./Canada market【482364198331002†L582-L595】,
  and ARPU remains highest in North America【482364198331002†L601-L613】.
* **Forecasting** – A simple ARIMA model forecasts global streaming revenue
  for the next four quarters.  The forecasting code can be extended to
  incorporate more sophisticated models (Prophet, XGBoost, LSTM, etc.).
* **Interactive dashboard** – A Streamlit app lets users select regions
  and metrics and view dynamic charts.  The dashboard also displays the
  revenue forecast.

The repository is structured for easy extension.  To adapt this project for
other datasets or more complex pipelines, swap out the CSV file and
adjust the ETL configuration accordingly.

## Dockerised deployment

The project includes a fully containerised analytics stack defined in
`docker-compose.yml`.  This stack brings together **Apache NiFi** for flow‑based
ETL, **Apache Kafka** for reliable streaming of records, **PostgreSQL** for
persistent storage, and **Apache Airflow** to orchestrate the end‑to‑end
process.  Each component runs in its own container and communicates on an
internal network.  To launch the entire stack:

```bash
cd project-1
docker compose up -d
```

This will start the following services:

* **PostgreSQL** (`postgres:15`) – database for storing cleaned records.  The
  credentials are defined in `docker-compose.yml` (`netflix`/`netflix`) and
  the database name is `netflixdb`.  You can connect to it on port 5432.
* **Zookeeper** and **Kafka** (Bitnami images) – Kafka listens on port 9092
  and is used as a message bus to move data from NiFi into Postgres.  Topics
  are auto‑created.
* **Apache NiFi** – the flow UI is available on <http://localhost:8080>.  A
  template is provided in `nifi/flow_template.xml` that reads the raw CSV,
  splits it into records, converts types and publishes each record to the
  Kafka topic `netflix`.
* **Apache Airflow** – comprised of a scheduler and a webserver (port 8081).
  The DAG in `airflow/dags/netflix_etl_dag.py` triggers the NiFi process
  group, optionally publishes the CSV to Kafka (if NiFi does not), and
  consumes messages from Kafka into Postgres.

### NiFi flow configuration

The `nifi/` folder contains a basic flow template (`flow_template.xml`) and
a README with setup instructions.  To deploy it:

1. Visit the NiFi UI at <http://localhost:8080> after starting the stack.
2. Click **Upload Template** and select `nifi/flow_template.xml`.  Drag the
   template onto the canvas.
3. Update the **GetFile** processor to point to the mounted CSV
   (`/opt/nifi/data/netflix_revenue_updated.csv` inside the container) and
   configure the **PublishKafkaRecord_2_6** processor with the topic name
   `netflix` and bootstrap server `kafka:9092`.
4. Start the process group; NiFi will ingest the CSV, convert each row
   into JSON and push it to Kafka.

### Airflow DAG

The Airflow DAG `netflix_etl_dag.py` demonstrates how to orchestrate the
pipeline using Python tasks:

* **`trigger_nifi_flow`** – calls the NiFi REST API to start the process
  group.  Replace the placeholder `process_group_id` with the ID from
  your NiFi process group (shown in the UI).
* **`produce_to_kafka`** – an optional helper that publishes the CSV to
  Kafka using `kafka-python`.  If you configure NiFi to handle this, you
  can remove this task.
* **`consume_from_kafka_and_load`** – reads messages from Kafka and
  inserts them into the `netflix_raw` table in Postgres using `psycopg2`.

The DAG is configured to run on demand (no schedule).  Access the Airflow
web UI at <http://localhost:8081>, unpause the DAG and trigger it manually.

### Notes

* **Kafka vs. RabbitMQ** – this project uses Kafka because of its strong
  integration with NiFi and scalability for streaming workloads.  If you
  prefer RabbitMQ, you can modify the flow to use NiFi’s `PublishAMQP`
  processor and adjust the Airflow tasks to use `pika` for consumption.
* The containers are configured for local development.  For production,
  consider adding authentication, persistent volumes and resource limits.

Feel free to extend the stack further – e.g., add **FastAPI** to serve
forecasts or integrate **Tableau** for richer BI dashboards.  The
dockerised architecture provides a solid foundation for demonstrating
professional data‑engineering and data‑science skills.
