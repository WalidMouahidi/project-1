# Netflix Revenue & Subscriber Analytics

This project demonstrates an end‑to‑end data‑engineering and data‑science
workflow on a publicly available Netflix revenue dataset.  It is designed to
showcase skills in ETL, data modelling, exploratory analysis, forecasting,
dashboard development and basic DevOps.

## Dataset

The data set (`data/netflix_revenue_updated.csv`) contains quarterly metrics
for Netflix from **Q1 2019 through Q1 2024**.  For each quarter it records:

* Global streaming revenue
* Regional streaming revenue for the **U.S./Canada (UCAN)**, **EMEA**, **Latin America (LATM)** and **Asia–Pacific (APAC)** regions
* Streaming membership counts by region
* Average Revenue Per User (ARPU) by region
* Total streaming memberships

Industry data show that the U.S./Canada region remains Netflix’s largest
revenue generator (about **44 % of all revenue in recent years**)【482364198331002†L582-L595】, while the **EMEA region now has the most subscribers**
(~93.9 million in 2024)【482364198331002†L690-L704】.  The U.S./Canada market also generates the **highest ARPU** (≈ $17.17 per user in 2024)
and Asia–Pacific the lowest【482364198331002†L601-L613】.

## Project structure

```
project-1/
├── data/
│   ├── netflix_revenue_updated.csv  # raw dataset
│   └── netflix.db                   # SQLite database created by ETL pipeline
├── etl/
│   └── etl_pipeline.py             # reads CSV, transforms and loads into SQLite
├── notebooks/
│   └── eda_forecasting.py          # EDA & forecasting script
├── app/
│   └── streamlit_app.py            # interactive dashboard
├── outputs/
│   ├── revenue_trends.png          # generated by analysis script
│   ├── membership_trends.png
│   ├── arpu_trends.png
│   └── global_revenue_forecast.png
├── requirements.txt                # Python dependencies
└── README.md                       # project documentation (this file)
```

## Quick start

1. **Install dependencies**

   Create a virtual environment (optional) and install required packages:

   ```bash
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Run the ETL pipeline**

   The ETL script will read the CSV file, unpivot regional data, create a
   star‑schema and load everything into a local SQLite database (`data/netflix.db`):

   ```bash
   python etl/etl_pipeline.py
   ```

3. **Perform analysis and generate forecasts**

   Execute the analysis script to produce summary plots and a simple ARIMA
   forecast for global streaming revenue.  Plots are saved into the
   `outputs/` directory.

   ```bash
   python notebooks/eda_forecasting.py
   ```

4. **Launch the dashboard**

   Run the Streamlit app to explore the data interactively.  Select a
   region and metric (revenue, memberships or ARPU) and view the
   corresponding trend.  A global revenue forecast is also displayed.

   ```bash
   streamlit run app/streamlit_app.py
   ```

## About this project

The objective is to illustrate a full analytics workflow:

* **Data engineering** – A Python ETL pipeline (orchestrated via a simple
  script, but ready to be scheduled in Airflow) cleans and unpivots the
  quarterly revenue data and loads it into a properly modelled database.
* **Exploratory analysis** – Python scripts generate descriptive charts and
  compute growth metrics, highlighting regional patterns.  For example,
  global streaming revenue climbed from about **$10 billion in 2019** to over
  **$17 billion in 2024** in the U.S./Canada market【482364198331002†L582-L595】,
  and ARPU remains highest in North America【482364198331002†L601-L613】.
* **Forecasting** – A simple ARIMA model forecasts global streaming revenue
  for the next four quarters.  The forecasting code can be extended to
  incorporate more sophisticated models (Prophet, XGBoost, LSTM, etc.).
* **Interactive dashboard** – A Streamlit app lets users select regions
  and metrics and view dynamic charts.  The dashboard also displays the
  revenue forecast.

The repository is structured for easy extension.  To adapt this project for
other datasets or more complex pipelines, swap out the CSV file and
adjust the ETL configuration accordingly.

## Deployment & next steps

Although this demonstration uses a local SQLite database, the same code can be
adapted to load data into Postgres, BigQuery or Snowflake.  The ETL
pipeline could be orchestrated by **Apache Airflow** and tested/deployed via
**Docker** and **GitHub Actions**.  Additional improvements include:

* Extending the forecasting module to support per‑region forecasts and
  cross‑validation.
* Integrating the model into a **FastAPI** microservice and exposing a REST
  endpoint.
* Creating a **Power BI** or **Tableau** dashboard for more polished
  visualisations.

Feel free to use and adapt this project in your portfolio.  The dataset and
analysis provide a concise yet comprehensive demonstration of data
engineering and data science skills.
